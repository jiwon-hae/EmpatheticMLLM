{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f08dfbe-3064-4915-8880-39d5097b6208",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3e0c94-91a2-47c3-aa00-6adbbef9a59d",
   "metadata": {},
   "source": [
    "Run the following commands from terminal\n",
    "\n",
    "```bash\n",
    "# create a Jupyter kernel and install required packages\n",
    "module purge\n",
    "mamba create -n ml python=3.10\n",
    "mamba activate ml\n",
    "pip install numpy pandas torch transformers\n",
    "mamba install -c conda-forge ipykernel\n",
    "python -m ipykernel install --user --name ml --display-name \"ml\"\n",
    "```\n",
    "\n",
    "```bash\n",
    "# clone ImageBind and install requirements\n",
    "git clone https://github.com/facebookresearch/ImageBind.git\n",
    "cd ImageBind\n",
    "pip install .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bae24af-cc21-40ec-b64b-f6b0dfaff905",
   "metadata": {},
   "source": [
    "# Download MELD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2337be67-3a8e-47f2-b3ee-e068cf76e042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-05 17:39:48--  https://huggingface.co/datasets/declare-lab/MELD/resolve/main/MELD.Raw.tar.gz\n",
      "Resolving huggingface.co (huggingface.co)... 18.164.174.55, 18.164.174.17, 18.164.174.23, ...\n",
      "Connecting to huggingface.co (huggingface.co)|18.164.174.55|:443... connected.\n",
      "302 Foundest sent, awaiting response... \n",
      "Location: https://cdn-lfs.hf.co/repos/e5/f9/e5f9b3280b3cb63549fb6376cd84286e8bf7cade60967f984e192dda5701e74b/a56b4407d574195cbce470d86f9c9d72fcfea59b0e34502ecd4babee4a5c613e?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27MELD.Raw.tar.gz%3B+filename%3D%22MELD.Raw.tar.gz%22%3B&response-content-type=application%2Fgzip&Expires=1741228788&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MTIyODc4OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9lNS9mOS9lNWY5YjMyODBiM2NiNjM1NDlmYjYzNzZjZDg0Mjg2ZThiZjdjYWRlNjA5NjdmOTg0ZTE5MmRkYTU3MDFlNzRiL2E1NmI0NDA3ZDU3NDE5NWNiY2U0NzBkODZmOWM5ZDcyZmNmZWE1OWIwZTM0NTAyZWNkNGJhYmVlNGE1YzYxM2U%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=u%7E1JCOiCpvdUguS3nmEBLav%7E6PUiXHnBW7fCyk%7EL3oY1C7FedheCdWF4MdmsKYa0v20R%7EErH%7EXUQs61i%7Edja3EuyKgiE4BMsa9q%7EKQWjOedfdOeeJtzrcNBQhdXnQLy-RLq7WCcJtAe-tfv1l-Ri%7E64OFkagW1lZLULdc3p7baQDo1s2%7EX0qldDSsDKG4f8Zl1uNB8JPM6n0z3YYcmgRuc-7BvzQ9UTbGLoWRyt7NBuP2z01A%7EfG3oG20QT48meDUzeNubTmP9n-yHUclMkc32G4z8zqux4JVkx8NZs6VCN4t5b03TcXGME4yAxfNgvOLvV-YNO2QaI1tdseaQB7wA__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
      "--2025-03-05 17:39:48--  https://cdn-lfs.hf.co/repos/e5/f9/e5f9b3280b3cb63549fb6376cd84286e8bf7cade60967f984e192dda5701e74b/a56b4407d574195cbce470d86f9c9d72fcfea59b0e34502ecd4babee4a5c613e?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27MELD.Raw.tar.gz%3B+filename%3D%22MELD.Raw.tar.gz%22%3B&response-content-type=application%2Fgzip&Expires=1741228788&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MTIyODc4OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9lNS9mOS9lNWY5YjMyODBiM2NiNjM1NDlmYjYzNzZjZDg0Mjg2ZThiZjdjYWRlNjA5NjdmOTg0ZTE5MmRkYTU3MDFlNzRiL2E1NmI0NDA3ZDU3NDE5NWNiY2U0NzBkODZmOWM5ZDcyZmNmZWE1OWIwZTM0NTAyZWNkNGJhYmVlNGE1YzYxM2U%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=u%7E1JCOiCpvdUguS3nmEBLav%7E6PUiXHnBW7fCyk%7EL3oY1C7FedheCdWF4MdmsKYa0v20R%7EErH%7EXUQs61i%7Edja3EuyKgiE4BMsa9q%7EKQWjOedfdOeeJtzrcNBQhdXnQLy-RLq7WCcJtAe-tfv1l-Ri%7E64OFkagW1lZLULdc3p7baQDo1s2%7EX0qldDSsDKG4f8Zl1uNB8JPM6n0z3YYcmgRuc-7BvzQ9UTbGLoWRyt7NBuP2z01A%7EfG3oG20QT48meDUzeNubTmP9n-yHUclMkc32G4z8zqux4JVkx8NZs6VCN4t5b03TcXGME4yAxfNgvOLvV-YNO2QaI1tdseaQB7wA__&Key-Pair-Id=K3RPWS32NSSJCE\n",
      "3.169.231.4, 3.169.231.115, 3.169.231.38, ...\n",
      "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|3.169.231.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10878146150 (10G) [application/gzip]\n",
      "Saving to: ‘/scratch1/hae/MELD.Raw.tar.gz’\n",
      "\n",
      "^CLD.Raw.tar.gz       2%[                    ] 266.01M   110MB/s               \n",
      "\n",
      "gzip: stdin: unexpected end of file\n",
      "tar: Unexpected EOF in archive\n",
      "tar: Unexpected EOF in archive\n",
      "tar: Error is not recoverable: exiting now\n"
     ]
    }
   ],
   "source": [
    "!wget -P /scratch1/$USER https://huggingface.co/datasets/declare-lab/MELD/resolve/main/MELD.Raw.tar.gz\n",
    "!tar -xzf /scratch1/$USER/MELD.Raw.tar.gz -C /scratch1/$USER\n",
    "!rm /scratch1/$USER/MELD.Raw.tar.gz\n",
    "\n",
    "!find /scratch1/$USER/MELD.Raw/ -type f -name '*.tar.gz' -exec tar -xzf {} -C /scratch1/$USER/MELD.Raw/ \\;\n",
    "!find /scratch1/$USER/MELD.Raw/ -type f -name '*.tar.gz' -exec rm {} \\;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95df0c5-fb50-477c-9091-c069815f84b5",
   "metadata": {},
   "source": [
    "# Prepare the Dataset\n",
    "\n",
    "- Read each .csv file (train, dev, test) and for each utterance (uniquely identified by `Dialogue_ID d_id` and `Utterance_ID u_id`), collect the following information:\n",
    "    - `Conversational_History`: Concenated `Utterance` for all `Utterance_ID` $\\leq$ `u_id` and `Dialogue_ID` = `d_id`\n",
    "    - `Video_Path`: MP4 filepath associated  for utterance `(d_id, u_id)`\n",
    "    - `Label`: The response to the utterance `(d_id, u_id)`, which would be the utterance for `(d_id, u_id+1)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cedefdfe-8992-4bc6-8d7d-fc3ba962c890",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "splits = ['train', 'dev', 'test']\n",
    "\n",
    "def prepare_split(split):\n",
    "    df = pd.read_csv(f'/scratch1/{os.environ.get(\"USER\")}/MELD.Raw/{split}_sent_emo.csv')\n",
    "\n",
    "    video_folders = {\n",
    "        'train': 'train_splits',\n",
    "        'dev': 'dev_splits_complete',\n",
    "        'test': 'output_repeated_splits_test'\n",
    "    }\n",
    "\n",
    "    data = []\n",
    "    dialogues_df = df.groupby('Dialogue_ID')\n",
    "    \n",
    "    for d_id, dialogue in dialogues_df:\n",
    "        conv_hist = \"\"\n",
    "\n",
    "        for _, row in dialogue.iterrows():\n",
    "            u_id = row['Utterance_ID']\n",
    "\n",
    "            # Only process up to second-to-last utterance in each dialogue (because last utterance does not have target/response)\n",
    "            if u_id == dialogue['Utterance_ID'].max():\n",
    "                break\n",
    "\n",
    "            conv_hist += row['Utterance'] + \" \"\n",
    "\n",
    "            try:\n",
    "                next_utterance = dialogue[dialogue['Utterance_ID'] == u_id + 1].iloc[0]\n",
    "            except IndexError:\n",
    "                continue\n",
    "                \n",
    "            label = next_utterance['Utterance']\n",
    "            \n",
    "            video_path = f'/scratch1/{os.environ.get(\"USER\")}/MELD.Raw/{video_folders[split]}/dia{d_id}_utt{u_id}.mp4'\n",
    "            \n",
    "            data.append({\n",
    "                'Dialogue_ID': d_id,\n",
    "                'Utterance_ID': u_id,\n",
    "                'Conversational_History': conv_hist.strip(),\n",
    "                'Video_Path': video_path,\n",
    "                'Label': label\n",
    "            })\n",
    "    \n",
    "    return data\n",
    "\n",
    "train_data = prepare_split('train')\n",
    "dev_data = prepare_split('dev')\n",
    "test_data = prepare_split('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee03d614-ef79-44c4-ab6c-c155d2fd1426",
   "metadata": {},
   "source": [
    "## Extract audio .wav from .mp4 files for each utterance\n",
    "\n",
    "Run the following commands from the terminal:\n",
    "```\n",
    "./extract_audio.sh /scratch1/$USER/MELD.Raw/train_splits/ /scratch1/$USER/MELD.Raw/train_audio\n",
    "./extract_audio.sh /scratch1/$USER/MELD.Raw/dev_splits_complete/ /scratch1/$USER/MELD.Raw/dev_audio\n",
    "./extract_audio.sh /scratch1/$USER/MELD.Raw/output_repeated_splits_test/ /scratch1/$USER/MELD.Raw/test_audio\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03257c18-04a4-47b3-975b-e51162d670f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_audio_to_split(split, data):\n",
    "    # filter out utterances that failed audio extraction\n",
    "    data[:] = [\n",
    "        u for u in data\n",
    "        if os.path.isfile(f'/scratch1/{os.environ.get(\"USER\")}/MELD.Raw/{split}_audio/dia{u[\"Dialogue_ID\"]}_utt{u[\"Utterance_ID\"]}.wav')\n",
    "    ]\n",
    "\n",
    "    # assign valid audio file paths to each utterance\n",
    "    for u in data:\n",
    "        d_id = u['Dialogue_ID']\n",
    "        u_id = u['Utterance_ID']\n",
    "\n",
    "        audio_path = f'/scratch1/{os.environ.get(\"USER\")}/MELD.Raw/{split}_audio/dia{d_id}_utt{u_id}.wav'\n",
    "        u['Audio_Path'] = audio_path\n",
    "\n",
    "add_audio_to_split('train', train_data)\n",
    "add_audio_to_split('dev', dev_data)\n",
    "add_audio_to_split('test', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3b6d4eb-5602-4718-9a74-1f8064d48260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/hae/.conda/envs/ml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MultimodalMELD(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        u = self.data[idx]\n",
    "\n",
    "        return {\n",
    "            'text': u['Conversational_History'],\n",
    "            'audio': u['Audio_Path'],\n",
    "            'video': u['Video_Path'],\n",
    "            'label': u['Label']\n",
    "        }\n",
    "\n",
    "train_dataset = MultimodalMELD(train_data)\n",
    "dev_dataset = MultimodalMELD(dev_data)\n",
    "test_dataset = MultimodalMELD(test_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=2, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a88e436-9b31-4526-95bc-011acd2a213b",
   "metadata": {},
   "source": [
    "# Define the Model\n",
    "\n",
    "**Forward Pass:**\n",
    "- Use frozen ImageBind model to generate joint (multimodal) embedding\n",
    "    - Add the embeddings for each modality to get unified embedding\n",
    "- Use projector to project joint embedding into token embedding space\n",
    "- Use decoder-only LLM model to generate predicted response to the utterance\n",
    "\n",
    "**Fine Tuning:**\n",
    "- Fine-tune projector and LLM with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35ba0797-6098-4e6c-98d8-43092dd2358c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/hae/.conda/envs/ml/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home1/hae/.conda/envs/ml/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0223,  0.0326,  0.0214,  ..., -0.0364, -0.0179,  0.0184]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imagebind import data\n",
    "import torch\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType\n",
    "\n",
    "#### RUN THIS CELL #####\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "inputs = {\n",
    "    ModalityType.TEXT: data.load_and_transform_text([train_dataset[0]['text']], device),\n",
    "    ModalityType.AUDIO: data.load_and_transform_audio_data([train_dataset[0]['audio']], device),\n",
    "    ModalityType.VISION: data.load_and_transform_video_data([train_dataset[0]['video']], device)\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model(inputs)\n",
    "\n",
    "embeddings[ModalityType.TEXT]\n",
    "embeddings[ModalityType.AUDIO]\n",
    "embeddings[ModalityType.VISION]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495aa0eb-1669-4f17-8239-e43ad96588d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "from imagebind import data\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType\n",
    "\n",
    "class EmpatheticMLLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmpatheticMLLM, self).__init__()\n",
    "        \n",
    "        self.imagebind = imagebind_model.imagebind_huge(pretrained=True)\n",
    "        self.imagebind.eval() # freeze imagebind model\n",
    "\n",
    "        self.llm = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "        self.projector = nn.Linear(1024, self.llm.config.n_embd)\n",
    "\n",
    "    def forward(self, x, trg=None):\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        inputs = {\n",
    "            ModalityType.TEXT: data.load_and_transform_text(x['text'], device),\n",
    "            ModalityType.AUDIO: data.load_and_transform_audio_data(x['audio'], device),\n",
    "            ModalityType.VISION: data.load_and_transform_video_data(x['video'], device)\n",
    "        }\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            modality_embeddings = self.imagebind(inputs)\n",
    "\n",
    "            # (bs, 1024) -- 1024 is d for ImageBind\n",
    "            joint_embedding = modality_embeddings[ModalityType.TEXT] \\\n",
    "                + modality_embeddings[ModalityType.AUDIO] \\\n",
    "                + modality_embeddings[ModalityType.VISION]\n",
    "\n",
    "        # (bs, 1, 768) -- 768 is d for GPT2\n",
    "        inputs_embeds = self.projector(joint_embedding).unsqueeze(1)\n",
    "\n",
    "        if trg is not None:\n",
    "            # (bs, T, V)\n",
    "            output_logits = []\n",
    "\n",
    "            for t in range(trg.size(1)):\n",
    "                t_embed = self.llm.transformer.wte.weight[t]\n",
    "\n",
    "                inputs_embeds = torch.cat([inputs_embeds, trg_embeds], dim=1)\n",
    "\n",
    "                output = self.llm(inputs_embeds=inputs_embeds)\n",
    "                output_logits.append(output.logits)\n",
    "\n",
    "        output_logits = torch.stack(output_logits, dim=1)\n",
    "        return output_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d38d141-7696-4450-b2d7-1a56ba0abc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = EmpatheticMLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c773e89d-29ba-4936-85fb-e0b86215d2d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1024])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "x = {\n",
    "    'text': batch['text'],\n",
    "    'audio': batch['audio'],\n",
    "    'video': batch['video']\n",
    "}\n",
    "\n",
    "outputs = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "454ceaba-0968-4927-809f-f44e9e6ef652",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'projected_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprojected_embedding\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'projected_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "projected_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7c04a6-9254-42ec-9b89-d912eca01cef",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b6451-d16a-457b-98a2-d21505316f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x = {\n",
    "            'text': batch['text'],\n",
    "            'audio': batch['audio'],\n",
    "            'video': batch['video']\n",
    "        }\n",
    "\n",
    "        logits = model(x)\n",
    "\n",
    "        loss = loss_fn()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
