{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf8353d5-5e0d-41ff-b295-03923ed2c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b60004c4-78aa-43ba-8add-08f4753379e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f25a586c-259e-40c2-a349-220d8548df5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1038/1038 [02:06<00:00,  8.23it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def prepare_split(split):\n",
    "    prompt = r\"\"\"### INSTRUCTIONS ###\n",
    "Continue the conversation by generating **only the next line** spoken by the indicated character.\n",
    "Your response must be empathetic, showing understanding or emotional attunement to the preceding dialogue.\n",
    "\n",
    "### EXAMPLE ###\n",
    "\n",
    "=== DIALOGUE HISTORY ===\n",
    "Rachel: Hey!\n",
    "Ross: Hi!\n",
    "Rachel: What are you doing here?\n",
    "Ross: Ah y'know, this building is on my paper route so I...\n",
    "Rachel: Oh.\n",
    "Ross: Hi.\n",
    "Rachel: Hi.\n",
    "Ross: How'd did it go?\n",
    "Rachel: Oh well, the woman I interviewed with was pretty tough, but y'know thank God Mark coached me, because once I started talking about the fall line, she got all happy and wouldn't shut up.\n",
    "Ross:\n",
    "\n",
    "=== RESPONSE ===\n",
    "That sounds like a huge relief.\n",
    "\n",
    "### TASK ###\n",
    "\n",
    "=== DIALOGUE HISTORY ===\n",
    "{dialogue_hist}\n",
    "\n",
    "=== RESPONSE ===\n",
    "\"\"\"\n",
    "\n",
    "    dataset_dir = '/project/msoleyma_1026/EmpatheticResponseGeneration'\n",
    "    dialogues_df = pd.read_csv(f'{dataset_dir}/MELD.Raw/{split}_sent_emo.csv').groupby('Dialogue_ID')\n",
    "    targets_df = pd.read_csv(f'{dataset_dir}/Targets/{split}_structured.csv')\n",
    "\n",
    "    data = []\n",
    "    for d_id, dialogue in tqdm(dialogues_df, total=len(dialogues_df)):\n",
    "        dialogue_hist = \"\"\n",
    "\n",
    "        for _, row in dialogue.iterrows():\n",
    "            u_id = row['Utterance_ID']\n",
    "\n",
    "            multimodal_embed_path = f'{dataset_dir}/ImagebindEmbeds/{split}/dia{d_id}_utt{u_id}.pt'\n",
    "\n",
    "            if not os.path.isfile(multimodal_embed_path):\n",
    "                continue\n",
    "            \n",
    "            dialogue_hist += f\"{row['Speaker']}: {row['Utterance']}\\n\"\n",
    "            multimodal_embed = torch.load(f'{dataset_dir}/ImagebindEmbeds/{split}/dia{d_id}_utt{u_id}.pt')\n",
    "            target_response = targets_df[(targets_df['Dialogue_ID'] == d_id) & (targets_df['Utterance_ID'] == u_id)]['Response'].values[0]\n",
    "\n",
    "            next_speaker = dialogue[dialogue['Utterance_ID'] == u_id + 1].iloc[0]['Speaker']\n",
    "            data.append({\n",
    "                'Multimodal_Embed': multimodal_embed,\n",
    "                'Prompt': prompt.format(dialogue_hist=f\"{dialogue_hist}{next_speaker}:\"),\n",
    "                'Target_Response': target_response\n",
    "            })\n",
    "    \n",
    "    return data\n",
    "\n",
    "train_data = prepare_split('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1488236-58d1-40da-a827-314360f9a184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MultimodalMELD(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        u = self.data[idx]\n",
    "\n",
    "        return {\n",
    "            'multimodal_embed': u['Multimodal_Embed'],\n",
    "            'prompt': u['Prompt'],\n",
    "            'target_response': u['Target_Response']\n",
    "        }\n",
    "\n",
    "train_dataset = MultimodalMELD(train_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4ba240c-d04a-479f-8ccc-5848dc747dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "HfFolder.save_token(\"HUGGING_FACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45733afa-e7a8-4b2c-9ddc-41978400cc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May  3 17:00:50 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  |   00000000:81:00.0 Off |                    0 |\n",
      "| N/A   26C    P0             42W /  300W |       1MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5076ea5e-4eb8-40a2-a4d7-c8ba5be22591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "class EmpatheticMLLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmpatheticMLLM, self).__init__()\n",
    "\n",
    "        # quantized Mistral-7B-Instruct-v0.3\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=\"float16\"\n",
    "        )\n",
    "        \n",
    "        model_id = 'meta-llama/Meta-Llama-3-8B-instruct'\n",
    "        \n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # project from ImageBind embedding space to Mistral embedding space\n",
    "        self.projector = nn.Linear(1024, self.llm.config.hidden_size) # 1024 -> 4096\n",
    "\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.1,\n",
    "            task_type='CAUSAL_LM'\n",
    "        )\n",
    "\n",
    "        self.llm = get_peft_model(self.llm, lora_config)\n",
    "\n",
    "    def forward(self, x, training=False):\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        # project to LLM embedding space\n",
    "        multimodal_embed = self.projector(x['multimodal_embed']).to(self.llm.dtype)\n",
    "\n",
    "        # tokenize prompt\n",
    "        prompt_tokenized = self.tokenizer(x['prompt'], return_tensors=\"pt\", padding=True, truncation=True, max_length=32768).to(device)\n",
    "        prompt_ids = prompt_tokenized.input_ids\n",
    "        prompt_attention_mask = prompt_tokenized.attention_mask\n",
    "\n",
    "        # embed prompt tokens\n",
    "        prompt_embeds = self.llm.model.model.embed_tokens(prompt_ids)\n",
    "\n",
    "        # concatenate: [multimodal ImageBind embedding] + [prompt embeddings]\n",
    "        inputs_embeds = torch.cat([multimodal_embed, prompt_embeds], dim=1)\n",
    "        attention_mask = torch.cat([torch.ones(multimodal_embed.size(0), 1, device=device), prompt_attention_mask], dim=1)\n",
    "\n",
    "        # if training, provide labels for supervised learning\n",
    "        if training:\n",
    "            # tokenize target response\n",
    "            target_tokenized = self.tokenizer(x['target_response'], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            target_ids = target_tokenized.input_ids\n",
    "\n",
    "            # embed target response tokens\n",
    "            target_embeds = self.llm.model.model.embed_tokens(target_ids)\n",
    "            \n",
    "            # concatenate target embeddings\n",
    "            inputs_embeds = torch.cat([inputs_embeds, target_embeds], dim=1)\n",
    "            \n",
    "            # ignore multimodal token + prompt tokens in loss calculation\n",
    "            bs = multimodal_embed.size(0)\n",
    "            mask_len = multimodal_embed.size(1) + prompt_embeds.size(1)\n",
    "            labels = torch.cat([torch.full((bs, mask_len), -100, device=device), target_ids], dim=1)\n",
    "\n",
    "            outputs = self.llm(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            return outputs\n",
    "\n",
    "        # otherwise, just generate output\n",
    "        else:\n",
    "            outputs = self.llm.generate(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=25\n",
    "            )\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4223af8-2674-47a8-8c7d-d4f3025d1e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95287f462d24fbc969ec86bf60f58bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = EmpatheticMLLM().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5394626-b249-4698-8c01-dae9a30ac435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/8572 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Epoch 1/5: 100%|██████████| 8572/8572 [22:44<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.1461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 8572/8572 [22:46<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.9438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 8572/8572 [22:45<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.8639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 8572/8572 [22:46<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.7889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 8572/8572 [22:47<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.7128\n",
      "Final model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "projector_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if \"projector\" in name:\n",
    "        projector_params.append(param)\n",
    "\n",
    "qlora_params = []\n",
    "for name, param in model.llm.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        qlora_params.append(param)\n",
    "    \n",
    "optimizer = AdamW(projector_params + qlora_params, lr=1e-5)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x = {\n",
    "            'multimodal_embed': batch['multimodal_embed'].to(device),\n",
    "            'prompt': batch['prompt'],\n",
    "            'target_response': batch['target_response']\n",
    "        }\n",
    "\n",
    "        output = model(x, training=True)\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"finetuned_mllm_llama_3.pth\")\n",
    "print(\"Final model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21357386-f90c-4400-856b-f860e235f325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
