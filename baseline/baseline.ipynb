{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6692e358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch, math, os, re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from bert_score import score\n",
    "from nltk import ngrams\n",
    "from peft import LoraConfig, get_peft_model, PeftConfig, PeftModel\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ff6b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "targets_df = pd.read_csv(\"targets/test_targets.csv\")\n",
    "structured_df = pd.read_csv(\"structured/test_structured.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48a716cd-001c-417a-bcc0-604c07bd4a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train inputs: 9989\n",
      "Dev inputs: 1109\n",
      "Test inputs: 2610\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "train_df = pd.read_csv(\"data/train_sent_emo.csv\")\n",
    "dev_df = pd.read_csv(\"data/dev_sent_emo.csv\")\n",
    "test_df = pd.read_csv(\"data/test_sent_emo.csv\")\n",
    "\n",
    "def create_formatted_inputs(df):\n",
    "    dialogues = defaultdict(list)\n",
    "\n",
    "    # Group utterances by dialogue\n",
    "    for _, row in df.iterrows():\n",
    "        dialogues[row[\"Dialogue_ID\"]].append((row[\"Speaker\"], row[\"Utterance\"], row[\"Emotion\"]))\n",
    "\n",
    "    # Convert to list format for easy processing\n",
    "    dialogues = list(dialogues.values())\n",
    "\n",
    "    # Generate formatted inputs with context\n",
    "    formatted_inputs = []\n",
    "    for dialogue in dialogues:\n",
    "        context = \"\"\n",
    "        for idx, (speaker, utterance, emotion) in enumerate(dialogue):\n",
    "            context += f\"{speaker}: {utterance}\\n\"\n",
    "\n",
    "            last_speaker = speaker\n",
    "            # Determine next speaker based on alternation if possible\n",
    "            if idx + 1 < len(dialogue):\n",
    "                next_speaker = dialogue[idx + 1][0]\n",
    "            else:\n",
    "                next_speaker = speaker  # Default to last speaker if no next available\n",
    "\n",
    "            prompt = r\"\"\"### INSTRUCTIONS ###\n",
    "Continue the conversation by generating **only the next line** spoken by the indicated character.\n",
    "Your response must be empathetic, showing understanding or emotional attunement to the preceding dialogue.\n",
    "\n",
    "### EXAMPLE ###\n",
    "\n",
    "=== DIALOGUE HISTORY ===\n",
    "Rachel: Hey!\n",
    "Ross: Hi!\n",
    "Rachel: What are you doing here?\n",
    "Ross: Ah y'know, this building is on my paper route so I...\n",
    "Rachel: Oh.\n",
    "Ross: Hi.\n",
    "Rachel: Hi.\n",
    "Ross: How'd did it go?\n",
    "Rachel: Oh well, the woman I interviewed with was pretty tough, but y'know thank God Mark coached me, because once I started talking about the fall line, she got all happy and wouldn't shut up.\n",
    "Ross:\n",
    "\n",
    "=== RESPONSE ===\n",
    "That sounds like a huge relief.\n",
    "\n",
    "### TASK ###\n",
    "\n",
    "=== DIALOGUE HISTORY ===\n",
    "{dialogue_hist}\n",
    "\n",
    "=== RESPONSE ===\n",
    "            \"\"\"\n",
    "\n",
    "            # full_input = (\n",
    "            #     \"### TASK ###\\n\"\n",
    "            #     \"Continue the conversation by generating **only one line** as the next speaker.\\n\"\n",
    "            #     \"This response should be **empathetic**, acknowledging or reflecting the emotional tone of the previous dialogue.\\n\"\n",
    "            #     \"DO NOT generate multiple lines.\\n\"\n",
    "            #     \"DO NOT summarize, analyze, or explain.\\n\"\n",
    "            #     \"Only generate one line and nothing more.\\n\\n\"\n",
    "            #     \"### DIALOGUE HISTORY ###\\n\"\n",
    "            #     f\"{context.strip()}\\n\"\n",
    "            #     f\"{next_speaker}:\"\n",
    "            # )\n",
    "\n",
    "            formatted_inputs.append(prompt.format(dialogue_hist=f\"{context}{next_speaker}:\"))\n",
    "\n",
    "    return formatted_inputs\n",
    "\n",
    "# Apply to each split\n",
    "train_formatted_inputs = create_formatted_inputs(train_df)\n",
    "dev_formatted_inputs = create_formatted_inputs(dev_df)\n",
    "test_formatted_inputs = create_formatted_inputs(test_df)\n",
    "\n",
    "print(f\"Train inputs: {len(train_formatted_inputs)}\")\n",
    "print(f\"Dev inputs: {len(dev_formatted_inputs)}\")\n",
    "print(f\"Test inputs: {len(test_formatted_inputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6e1a6df-cde9-4904-adc8-42f6959bd4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### INSTRUCTIONS ###\n",
      "Continue the conversation by generating **only the next line** spoken by the indicated character.\n",
      "Your response must be empathetic, showing understanding or emotional attunement to the preceding dialogue.\n",
      "\n",
      "### EXAMPLE ###\n",
      "\n",
      "=== DIALOGUE HISTORY ===\n",
      "Rachel: Hey!\n",
      "Ross: Hi!\n",
      "Rachel: What are you doing here?\n",
      "Ross: Ah y'know, this building is on my paper route so I...\n",
      "Rachel: Oh.\n",
      "Ross: Hi.\n",
      "Rachel: Hi.\n",
      "Ross: How'd did it go?\n",
      "Rachel: Oh well, the woman I interviewed with was pretty tough, but y'know thank God Mark coached me, because once I started talking about the fall line, she got all happy and wouldn't shut up.\n",
      "Ross:\n",
      "\n",
      "=== RESPONSE ===\n",
      "That sounds like a huge relief.\n",
      "\n",
      "### TASK ###\n",
      "\n",
      "=== DIALOGUE HISTORY ===\n",
      "Chandler: also I was the point person on my company’s transition from the KL-5 to GR-6 system.\n",
      "The Interviewer:\n",
      "\n",
      "=== RESPONSE ===\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "print(train_formatted_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59e5e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.99s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32768, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): MistralRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model/tokenizer\n",
    "os.environ[\"HF_TOKEN\"] = \"HUGGING_FACE_TOKEN\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f35b12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|██████████| 2610/2610 [37:49<00:00,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate responses\n",
    "generated_responses = []\n",
    "first_sentences = []\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "sentence_splitter = re.compile(r\"(?<=[\\.\\?!…])\\s+\")\n",
    "for prompt in tqdm(test_formatted_inputs, desc=\"Generating responses\"):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=25,\n",
    "            # do_sample=True,\n",
    "            # top_k=50,\n",
    "            # top_p=0.95,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Remove prompt from beginning if present\n",
    "    if generated.startswith(prompt):\n",
    "        generated = generated[len(prompt):].strip()\n",
    "    generated_responses.append(generated)\n",
    "\n",
    "    # Extract the first sentence using regex\n",
    "    sentences = sentence_splitter.split(generated.strip())\n",
    "    first_sentence = sentences[0].strip() if sentences else \"\"\n",
    "    first_sentences.append(first_sentence)\n",
    "\n",
    "# Match Dialogue_ID and Utterance_ID\n",
    "structured_ids = structured_df[[\"Dialogue_ID\", \"Utterance_ID\"]]\n",
    "baseline_df = structured_ids.copy()\n",
    "baseline_df[\"Response\"] = first_sentences\n",
    "baseline_df[\"Original Response\"] = generated_responses\n",
    "\n",
    "# Save to file\n",
    "os.makedirs(\"baseline\", exist_ok=True)\n",
    "baseline_df.to_csv(\"baseline/mistral_25_test_baseline.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e4fd92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Perplexity\n",
    "def compute_perplexity(sentences):\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        enc = tokenizer.encode(sent, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(enc, labels=enc)\n",
    "        loss = outputs.loss  # This is *average loss per token* for the sentence\n",
    "        num_tokens = enc.size(1)  # Sequence length\n",
    "\n",
    "        total_loss += loss.item() * num_tokens  # Recover total loss for all tokens\n",
    "        total_tokens += num_tokens\n",
    "\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    ppl = math.exp(avg_loss)\n",
    "    return ppl\n",
    "\n",
    "\n",
    "# 2. Distinct-n\n",
    "def compute_dist_n(sentences, n):\n",
    "    all_ngrams = []\n",
    "    for sent in sentences:\n",
    "        tokens = sent.split()\n",
    "        all_ngrams.extend(ngrams(tokens, n))\n",
    "    total = len(all_ngrams)\n",
    "    unique = len(set(all_ngrams))\n",
    "    return unique / total if total > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fb0bbb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Get targets\n",
    "reference_responses = structured_df[\"Response\"].tolist()\n",
    "\n",
    "trimmed_responses = baseline_df[\"Response\"].tolist()\n",
    "\n",
    "# Metrics computation\n",
    "ppl = compute_perplexity(trimmed_responses)\n",
    "dist1 = compute_dist_n(trimmed_responses, 1)\n",
    "dist2 = compute_dist_n(trimmed_responses, 2)\n",
    "# bertscore = compute_bertscore(generated_responses, reference_responses)\n",
    "P, R, F1 = score(trimmed_responses, reference_responses, lang=\"en\")\n",
    "bertscore = {\n",
    "    \"precision\": P.mean().item(),\n",
    "    \"recall\": R.mean().item(),\n",
    "    \"f1\": F1.mean().item()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e28c6687-10ed-43cd-b70e-ef694fd11135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 23.559440080183258\n",
      "Dist-1: 0.09818864590236359\n",
      "Dist-2: 0.31100787198348173\n",
      "BERTScore: {'precision': 0.8756473660469055, 'recall': 0.8778427243232727, 'f1': 0.8765256404876709}\n"
     ]
    }
   ],
   "source": [
    "# Collect all metrics\n",
    "metrics = {\n",
    "    \"Perplexity\": ppl,\n",
    "    \"Dist-1\": dist1,\n",
    "    \"Dist-2\": dist2,\n",
    "    \"BERTScore\": bertscore\n",
    "}\n",
    "\n",
    "# Print to console\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}:\", v)\n",
    "\n",
    "# Save to JSON\n",
    "os.makedirs(\"baseline\", exist_ok=True)\n",
    "with open(\"baseline/mistral_25_baseline_metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d5e560-9b9c-41ad-bc8b-3f253a821ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.69s/it]\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"HUGGING_FACE_TOKEN\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    # bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f03f24c-3073-4840-96b3-29d68a21c556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9989/9989 [00:04<00:00, 2154.05 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess your dataset\n",
    "df = pd.read_csv(\"targets/train_targets.csv\")\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Attach train_formatted_inputs directly to the dataset\n",
    "dataset = dataset.add_column(\"Formatted_Prompt\", train_formatted_inputs)\n",
    "\n",
    "def tokenize(batch):\n",
    "    texts = [\n",
    "        prompt + tokenizer.eos_token + response\n",
    "        for prompt, response in zip(batch[\"Formatted_Prompt\"], batch[\"Response\"])\n",
    "    ]\n",
    "    return tokenizer(texts, truncation=True, padding=\"max_length\", max_length=2048)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "896ad1a5-7ca2-4b25-8537-e9f3d6254913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"mistral-lora-checkpoints\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4766f872-6dc5-4637-ab1d-4998c1b3b418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='395' max='624' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [395/624 2:34:39 < 1:30:07, 0.04 it/s, Epoch 0.63/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.358800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.719800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.683700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.674300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.679300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.652200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.650900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Save\n",
    "model.save_pretrained(\"mistral-lora-finetuned\")\n",
    "tokenizer.save_pretrained(\"mistral-lora-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9166d4bc-d2a4-41c3-a686-e749bf6bbee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [02:12<00:00, 44.03s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32768, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): MistralRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer and base model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistral-lora-finetuned\", trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load PEFT config and fine-tuned model\n",
    "config = PeftConfig.from_pretrained(\"mistral-lora-finetuned\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, \"mistral-lora-finetuned\")\n",
    "\n",
    "# Move to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7320b205-bfc6-4e17-b33b-ba23d2ddb3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|██████████| 2610/2610 [32:54<00:00,  1.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate responses\n",
    "targets_df = pd.read_csv(\"targets/test_targets.csv\")\n",
    "structured_df = pd.read_csv(\"structured/test_structured.csv\")\n",
    "\n",
    "generated_responses = []\n",
    "first_sentences = []\n",
    "\n",
    "sentence_splitter = re.compile(r\"(?<=[\\.\\?!…])\\s+\")\n",
    "for prompt in tqdm(test_formatted_inputs, desc=\"Generating responses\"):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=25,\n",
    "            # do_sample=True,\n",
    "            # top_k=50,\n",
    "            # top_p=0.95,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if generated.startswith(prompt):\n",
    "        generated = generated[len(prompt):].strip()\n",
    "    generated_responses.append(generated)\n",
    "\n",
    "    sentences = sentence_splitter.split(generated.strip())\n",
    "    first_sentence = sentences[0].strip() if sentences else \"\"\n",
    "    first_sentences.append(first_sentence)\n",
    "\n",
    "# Match Dialogue_ID and Utterance_ID\n",
    "structured_ids = structured_df[[\"Dialogue_ID\", \"Utterance_ID\"]]\n",
    "baseline_df = structured_ids.copy()\n",
    "baseline_df[\"Response\"] = first_sentences\n",
    "baseline_df[\"Original Response\"] = generated_responses\n",
    "\n",
    "# Save to file\n",
    "os.makedirs(\"baseline\", exist_ok=True)\n",
    "baseline_df.to_csv(\"baseline/mistral_test_finetuned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16d28434-6031-409d-bbe0-d6fcb557e6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Get targets\n",
    "reference_responses = structured_df[\"Response\"].tolist()\n",
    "\n",
    "trimmed_responses = baseline_df[\"Response\"].tolist()\n",
    "\n",
    "# Metrics computation\n",
    "ppl = compute_perplexity(trimmed_responses)\n",
    "dist1 = compute_dist_n(trimmed_responses, 1)\n",
    "dist2 = compute_dist_n(trimmed_responses, 2)\n",
    "# bertscore = compute_bertscore(generated_responses, reference_responses)\n",
    "P, R, F1 = score(trimmed_responses, reference_responses, lang=\"en\")\n",
    "bertscore = {\n",
    "    \"precision\": P.mean().item(),\n",
    "    \"recall\": R.mean().item(),\n",
    "    \"f1\": F1.mean().item()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a30d64b-d1f7-465f-a8d1-bcb8ce04b0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 17.815717100430387\n",
      "Dist-1: 0.06838336727320501\n",
      "Dist-2: 0.19654324902877984\n",
      "BERTScore: {'precision': 0.8906997442245483, 'recall': 0.8879204392433167, 'f1': 0.8890686631202698}\n"
     ]
    }
   ],
   "source": [
    "# Collect all metrics\n",
    "metrics = {\n",
    "    \"Perplexity\": ppl,\n",
    "    \"Dist-1\": dist1,\n",
    "    \"Dist-2\": dist2,\n",
    "    \"BERTScore\": bertscore\n",
    "}\n",
    "\n",
    "# Print to console\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}:\", v)\n",
    "\n",
    "# Save to JSON\n",
    "os.makedirs(\"baseline\", exist_ok=True)\n",
    "with open(\"baseline/mistral_finetuned_metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1345d4-f3ac-4bce-b521-0bba9dbadab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
